\documentclass[14pt,xcolor=dvipsnames]{beamer}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{shapes,arrows}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\pgfplotsset{compat=1.17}
\usefonttheme{professionalfonts}
\usetikzlibrary{shapes,arrows}
\newcommand{\mx}[1]{\mathbf{\bm{#1}}} % Matrix command
\newcommand{\vc}[1]{\mathbf{\bm{#1}}} % Vector command
\usepackage{amsmath,enumerate,xspace,bbold,amsthm,arydshln,bm}
\usepackage{multirow,array,theorem,amsmath,latexsym,xspace, bibentry,tikz}
\usepackage{array,theorem,amsmath,latexsym,hhline,pifont,xspace, bibentry,epstopdf, comment,dsfont}
%\usepackage{algorithm,algorithmic,comment}
\usepackage{algorithm,algpseudocode}
\usepackage[round,authoryear]{natbib}
\usepackage{grffile}
\usepackage{booktabs}
\usepackage{graphicx}
\usetikzlibrary{tikzmark,arrows}
\usetikzlibrary{positioning,arrows.meta}
\usetheme{AnnArbor}
\setbeamerfont*{itemize/enumerate subbody}{parent=itemize/enumerate body}
\setbeamerfont*{itemize/enumerate subsubbody}{parent=itemize/enumerate body}
\setbeamercolor{alerted text}{fg=red!80!yellow}
\newcommand{\libsvm}{$\mbox{\href{http://www.csie.ntu.edu.tw/~cjlin/libsvm}{\sf LIBSVM}}$\xspace}
\newcommand{\liblinear}{$\mbox{\href{http://www.csie.ntu.edu.tw/~cjlin/liblinear}{\sf LIBLINEAR}}$\xspace}
\usetikzlibrary{matrix}
\usetikzlibrary{fit}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathreplacing}
\def\R{{ \mathbf{R}}}
\def\Z{{ \mathbf{Z}}}
\def\ba{{\boldsymbol a}}
\def\bc{{\boldsymbol c}}
\def\bg{{\boldsymbol g}}
\def\bt{{\boldsymbol t}}
\def\bx{{\boldsymbol x}}
\def\by{{\boldsymbol y}}
\def\bi{{\boldsymbol I}}
\def\br{{\boldsymbol r}}
\def\bz{{\boldsymbol z}}
\def\bv{{\boldsymbol v}}
\def\bb{{\boldsymbol b}}
\def\bp{{\boldsymbol p}}
\def\bq{{\boldsymbol q}}
\def\bw{{\boldsymbol w}}
\def\bu{{\boldsymbol u}}
\def\bv{{\boldsymbol v}}
\def\bs{{\boldsymbol s}}
\def\be{{\boldsymbol e}}
\def\bd{{\boldsymbol d}}
\def\b1{{\boldsymbol 1}}
\def\bk{{\boldsymbol k}}
\def\bzero{{\boldsymbol 0}}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\AL{{\boldsymbol{\alpha}}}
\def\ALL{{\boldsymbol{\alpha'}}}
\def\BL{\boldsymbol \beta}
\def\boldgamma{\boldsymbol \gamma}
\def\bla{\boldsymbol \lambda}
\def\bnu{\boldsymbol \nu}
\def\bmu{\boldsymbol \mu}
\def\boldeta{\boldsymbol \eta}
\def\tron{{\sf TRON}\xspace}
\def\vec{{\text{vec}}}
\def\mat{{\text{mat}}}
\def\mfirst{m}
\def\msec{{m+1}}
\def\mQ{\mathbf{Q}}
\def\mK{\mathbf{K}}
\def\mV{\mathbf{V}}
\def\mO{\mathbf{O}}
\def\mS{\mathbf{S}}
\def\mP{\mathbf{P}}
% Force numbering to start from 0
\setcounter{algorithm}{-1}

\setbeamertemplate{caption}[numbered]

\newcommand{\rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\AtBeginSection[]
{
	\begin{frame}<beamer>
		\frametitle{Outline}
		\tableofcontents[current,currentsection]
	\end{frame}
}

\tikzset{
    neuron/.style={circle, line width=1.2pt, draw, minimum size=8mm},
    crossed/.style={circle, line width=1.2pt, draw, minimum size=8mm, dashed},
    connect/.style={-latex, line width=1.2pt},
}

\begin{document}
	\tikzset{
		arr/.style={draw opacity=1,
			decoration={markings,mark=at position 1
				with {\arrow[scale=2]{>}}},
			postaction={decorate}, shorten >=0.4pt,
		}
	}
	
	% \title[]{Some Notes on Automatic Differentiation}
	% \author[Chih-Jen Lin]{{\large Chih-Jen Lin}\\
	% 	% Department of Computer Science\\
	% 	National Taiwan University \\
	% 	\bigskip
	% 	Last updated: \today
	% 	%\centering\epsfig{figure = ../../ntulogo.eps, width = 2.5cm} 
	% 	%\includegraphics[width=1.85cm]{./ntulogo.png}
	% 	% \\
	% 	% {\small 
	% 	% Joint work with Chien-Chih Wang, Kent Loong Tan, Chun-Ting
	% 	% Chen (National Taiwan University), and Sathiya Keerthi, Dhruv
	% 	% Mahajan, Sundararajan Sellamanickam (Microsoft)
	% 	% }
	% }
	
	% \institute[National Taiwan Univ.]{}
	% \date[]{}
	% % \begin{frame}
	% %    \titlepage
	% % \end{frame}
	% %\logo{\includegraphics[width=0.5cm]{../../../latex/ntulogo.png}}

\title[]{An Introduction to FlashAttention}
\author[Chih-Jen Lin]{{\large Chih-Jen Lin}\\
% Department of Computer Science\\
National Taiwan University, MBZUAI \\
\bigskip
Last updated: \today
}

\institute[National Taiwan Univ.]{}
\date[]{}
\begin{frame}
   \titlepage
\end{frame}

%\begin{frame}
%  \frametitle{Outline}
%  \tableofcontents[hideallsubsections]
%\end{frame}

\begin{frame}[allowframebreaks]
	\frametitle{Inefficiency of Attention Operation}
	\begin{itemize}
		\item Similar to the memory-access issue discussed
          above for matrix-matrix products, a
          possible bottleneck of attention is
          on moving data (i.e., matrices) between
          lower-level and upper-level memory on GPUs.
        \item To analyze this issue, we must check the number
          of memory accesses.
	\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
  \frametitle{Attention Operations}
  \begin{itemize}
  \item For the discussion, first we recall details
    of attention. For simplicity, we consider
    the single-head attention.
\item If the input matrix is
  \begin{equation*}
    \tilde{Z} \in \R^{T \times d},
  \end{equation*}
  the attention operation is
    \begin{equation}
      \label{eq:attention1}
\text{SoftMax} (\frac{\tilde{Z} W_Q W_K^\top (\tilde{Z})^\top}{\sqrt{d}}) \tilde{Z} W_V.
    \end{equation}
  \item We consider three
 trainable weight matrices
    \begin{equation*}
      W_Q \in \R^{d \times d}, W_K \in \R^{d \times d}, W_V \in \R^{d \times d}
    \end{equation*}
    to convert the input matrix $\tilde{Z}$ to
    \begin{equation*}
\tilde{Z} W_Q \in \R^{T\times d}, \quad \tilde{Z} W_K \in \R^{T\times d}, \quad \tilde{Z} W_V \in \R^{T\times d}.
\end{equation*}
  \item In \eqref{eq:attention1}, the \text{SoftMax} function
    is applied on each row $\bz$ of an input matrix in 
the following way.
    \begin{equation}
      \label{eq:softmax}
      \text{SoftMax}(\textbf{z}) = \begin{bmatrix} \frac{\exp(z_1)}{\sum_j\exp(z_j)}\\\vdots\\\frac{\exp(z_T)}{\sum_j\exp(z_j)}\end{bmatrix}.
    \end{equation}
  \item FlashAttention \citep{TD22a} defines that
  \begin{equation*}
  	\mQ := \tilde{Z} W_Q, \quad \mK := \tilde{Z} W_K, \quad \mV := \tilde{Z} W_V,
  \end{equation*}
  and assumes that $\mQ, \mK, \mV$ had been already precomputed.
  \item Omitting $1/\sqrt{d}$ for simplification, FlashAttention turns \eqref{eq:attention1} into
  \begin{equation}
  \label{eq:attention2}
  	\mO := \text{SoftMax} (\mQ \mK^\top) \mV,
  \end{equation}
  where $\mO \in \R^{T\times d}$ is the output matrix of the attention.
  \end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
  \frametitle{Memory Accesses in Attention}
  \begin{itemize}
  \item We still assume that our machine has
only two layers of memory:
  \begin{itemize}
  \item main memory, and
  \item secondary memory.
  \end{itemize}
\item If an operand is not 
available in main memory, we must transport it from secondary 
memory.
\item Now consider \eqref{eq:attention2} and check intermediate
  values during computation.
\item We need
  \begin{gather}
\mQ \mK^\top \in R^{T \times T} \label{eq:attention_op1}\\
\text{SoftMax} (\mQ \mK^\top) \in R^{T \times T} \label{eq:attention_op2}\\
\text{SoftMax} (\mQ \mK^\top) \mV \in R^{T \times d} \label{eq:attention_op3}
  \end{gather}
\item As in general
  \begin{equation*}
    T \gg d,
  \end{equation*}
even though the output
  \begin{equation*}
    \text{SoftMax} (\mQ \mK^\top) \mV \in R^{T \times d}
  \end{equation*}
is smaller,  storing $T\times T$ intermediate matrices is the main
  difficulty.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Insufficient Memory to Store $T \times T$
  Intermediate Matrices}
  \begin{itemize}
\item Our first analysis is to assume that
  \begin{equation*}
    T \times T
  \end{equation*}
  matrices cannot be stored in the main memory, and check the
  need to move these matrices
\item If we consider
  \eqref{eq:attention_op1}-\eqref{eq:attention_op3} as
  independent operations, immediately we see the following
  major memory accesses:
  \begin{itemize}
  \item write
    \begin{equation}\label{eq:attention_mat1}
\mQ \mK^\top \in R^{T \times T}
\end{equation}
to secondary memory,
\item load the matrix \eqref{eq:attention_mat1} from secondary memory to calculate
  \begin{equation}
    \label{eq:attention_mat2}
\text{SoftMax} (\mQ \mK^\top) 
\end{equation}
and write results back to secondary memory, and
\item load the matrix in \eqref{eq:attention_mat2}
  for calculating 
  \begin{equation*}
\text{SoftMax} (\mQ \mK^\top) \mV \in R^{T \times d}.
\end{equation*}
\end{itemize}
\item We assume that even though storing an $T\times T$
  matrix in main memory is not possible, the computer has
  a way to sequentially work on part of the
  input data in main memory and gradually generate/save part of the whole output to secondary memory
\item [] It is just like that we do matrix-matrix products
  all the time, but never worry that our highest-level
  memory (i.e., registers) is insufficient to store
  operands.
\item Now we conclude that a naive implementation of attention
  leads to
  \begin{equation}
    \label{eq:4Tsq}
    4 \times T^2
  \end{equation}
  accesses between main and secondary memory.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
	\frametitle{Memory Versus Computation}
        \begin{itemize}
        \item We see attention involves the following
          operations and list their respective cost.
  \begin{gather*}
\mQ \mK^\top: 2 T^2 d, \\
\text{SoftMax} (\mQ \mK^\top): 3T^2,
\\
\text{SoftMax} (\mQ \mK^\top) \mV: 2T^2d.
  \end{gather*}
\item Clearly, if
  \begin{gather*}
    4T^2 d \times \text{ cost per operation}\\
    <
    \\
    4T^2 \times \text{ cost per memory access},
  \end{gather*}
  then attention is \alert{memory bounded}.
\item Example:
  \begin{itemize}
    \item Consider $T=1024$ and $d=64$, running on an A100 80GB SXM GPU. 
    \item This GPU sustains up to 312 tera floating point operations per second (TFLOPS) for half-precision floating-point format (FP16) operations.
    \item The GPU is equipped with 80 GB of high-bandwidth memory (HBM), corresponding to the secondary memory in our setting and offering a memory bandwidth of 2.04 tera bytes per second (TB/s).
    \item Then, the total compute cost is
    \begin{equation*}
      \frac{4 \times 1024^2 \times 64 \ \text{FLOPs}}{312 \ \text{TFLOPS}} 
      \;\approx\; 0.86~\mu\text{s}.
    \end{equation*}
    \item The total memory access cost is
    \begin{equation*}
      \frac{4 \times 1024^2 \times 2 \ \text{bytes}}{2.04 \ \text{TB/s}} 
      \;\approx\; 3.74~\mu\text{s}.
    \end{equation*}
    Note that we assume each value is stored in FP16 (two bytes).
    % \item Clearly, $3.74~\mu\text{s} \gg 0.86~\mu\text{s}$, 
    % indicating that the runtime is dominated by memory accessing.
    % \item Thus, with FP16 operations on the A100 80GB SXM and $T=1024$, $d=64$, 
    % the attention layer is severely \textbf{memory-bounded}.
  \end{itemize}
\item To accelerate attention, we must reduce the number of memory accesses.
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
	\frametitle{Situation When Storing $T \times d$ Matrices Are Possible}
        \begin{itemize}
        \item Now we assume that the main memory is sufficient to store
          \begin{equation*}
            T\times d \text{ matrices such as }
            Q, K, \text{ and } V. 
          \end{equation*}
        \item Let us develop strategies to reduce the
          number of memory accesses in \eqref{eq:4Tsq}.
        \item In particular, we would like to avoid loading
          and storing intermediate results.
        \item That is, we should generate the attention
          results ``part by part.'' All we need is to
          sequentially store the finished part back
          to the secondary memory.
        \item We can conduct the following procedure:
\begin{itemize}
\item Load $Q, K, \text{ and } V$ to main memory.
\item For $i = 1, \ldots, T$, calculate
  \begin{gather}
\mQ_{i,:} \mK^\top \in R^{1\times T} \label{eq:Nd1}\\
\text{SoftMax} (\mQ_{i,:} \mK^\top) \in R^{1\times T} \label{eq:Nd2}\\
\text{SoftMax} (\mQ_{i,:} \mK^\top) \mV \in R^{1\times d} \label{eq:Nd3}
\end{gather}
and store the $i$th row of the output matrix to the secondary memory.
\end{itemize}
\item By this way, the number of memory accesses is reduced
  to
  \begin{equation*}
    O(Td).
  \end{equation*}
  because we never load/store any $T\times T$ matrices.
        \end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
	\frametitle{Situation When Storing $T \times d$ Matrices Are Not Possible}
        \begin{itemize}
\item Unfortunately, our assumption of that $T \times d$
  matrices can be stored in main memory is often untrue.
\item We discuss the situation of
  \begin{equation*}
    d \leq M \leq Td,
  \end{equation*}
    where $M$
  is the size of the main memory.

\item Then in \eqref{eq:Nd1}-\eqref{eq:Nd3}, we must load the
  whole $K$ and $V$ once
  even for calculating just one output row.
\item A possible strategy is to calculate $|I|$ rows together:
  \begin{equation}\label{eq:QK}
    \text{SoftMax} (\mQ_{I,:} \mK^\top) \mV,
  \end{equation}
  where $I$ is the block of rows that we intend to calculate.
\item In calculating \eqref{eq:QK}, we must access
  \begin{center}
    $|I|$ rows of $Q$, and the whole $K$ and $V$ (by row blocks). 
  \end{center}
  
\item We also need to store the intermediate block of
  \begin{equation}
    \label{eq:intermediate}
    \mQ_{I,:} \mK^\top,
  \end{equation}
  which requires
  \begin{equation*}
    |I| \times T
  \end{equation*}
  space.
\item The largest possible $|I|$ is
  \begin{equation*}
    \frac{M}{T},
  \end{equation*}
  where $M$
  is the size of the main memory.
\item Thus the total number of memory accesses is
  \begin{equation}
    O(\frac{T}{M/T}) \times O(Td)
    = O(\frac{T^3 d}{M}).\label{eq:Ncubic}
  \end{equation}
\item In the above discussion, we see that the main bottleneck is to
  store the intermediate matrix in \eqref{eq:intermediate}.
\item Because the number of rows in \eqref{eq:intermediate}
  is a large number $T$,
  $|I|$ must be small. Thus we get a large first term in \eqref{eq:Ncubic}.
        \end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
	\frametitle{FlashAttention}
        \begin{itemize}
\item To reduce the number of memory accesses, let us see if we may avoid storing the intermediate matrix in \eqref{eq:intermediate}.
\item Assume that we split $Q$ to the following row-block form:
  \begin{equation*}
    \begin{bmatrix}
      Q_{I_1,:}\\
      \vdots\\
      Q_{I_{T_r},:}
    \end{bmatrix}
  \end{equation*}
with
  \begin{equation*}
    |I_1| = \cdots = |I_{T_r}|.
  \end{equation*}
\item For $K, V$, we respectively split them to
    \begin{equation*}
    \begin{bmatrix}
      K_{J_1,:}\\
      \vdots\\
      K_{J_{T_c},:}
    \end{bmatrix},
        \begin{bmatrix}
      V_{J_1,:}\\
      \vdots\\
      V_{J_{T_c},:}
    \end{bmatrix}.
  \end{equation*}
\item We explain later why the split of $Q$ should be
  different from that of $K, V$.
\item In our discussion, we let $I, J$ be any one of
  $  |I_1|, \ldots, |I_{T_r}|$ and
    $  |J_1|, \ldots, |J_{T_c}|$, respectively.
  \item We have
    \begin{equation*}
      \begin{split}
&        \text{SoftMax} (\mQ_{I,:}
        \begin{bmatrix}
          \mK_{J_1,:}^\top & \cdots & \mK_{J_{T_c},:}^\top
        \end{bmatrix}
        )
        \begin{bmatrix}
          \mV_{J_1,:} \\ \vdots \\ \mV_{J_{T_c},:}
        \end{bmatrix}
  \\
        = &
        \text{SoftMax} (
        \begin{bmatrix}
          \mQ_{I,:}\mK_{J_1,:}^\top & \cdots & \mQ_{I,:}\mK_{J_{T_c},:}^\top
        \end{bmatrix}
        )
        \begin{bmatrix}
          \mV_{J_1,:} \\ \vdots \\ \mV_{J_{T_c},:}
        \end{bmatrix}
       \end{split}
    \end{equation*}
  \item If there is no SoftMax, we can see the result is
 \begin{equation*}
    (\mQ_{I,:}   \mK_{J_1,:}^\top)  \mV_{J_1,:} +
 \cdots + 
  (\mQ_{I,:}\mK_{J_{T_c},:}^\top) \mV_{J_{T_c},:}.
 \end{equation*}
\item We have
  \begin{gather*}
    (\mQ_{I,:}   \mK_{J_1,:}^\top)  \mV_{J_1,:} \in |I|\times d,
    \\
    \vdots
    \\
    (\mQ_{I,:}\mK_{J_{T_c},:}^\top) \mV_{J_{T_c},:} \in |I|\times d.
  \end{gather*}

\item If we sequentially generate each term, there is no need to
  store the intermediate sub-matrix in \eqref{eq:intermediate}.
\item In this situation, the algorithm can be as follows.
\item For $i = 1, \ldots, T_r$
  \begin{itemize}
  \item For $j = 1, \ldots, T_c$
    \begin{equation}\label{eq:updateO}
O_{I_i,:} = O_{I_i,:} +  (\mQ_{I_i,:}   \mK_{J_j,:}^\top)  \mV_{J_j,:}.
    \end{equation}
  \item Save $O_{I_i,:}$ to secondary memory.
  \end{itemize}
\item At any time point we must store the following \alert{four} blocks in main memory
  \begin{enumerate}
\item $Q_{I,:} \in R^{|I|\times d}$
  \item $O_{I,:} \in R^{|I|\times d}$
  \item $K_{J,:} \in R^{|J|\times d}$ \alert{or} $V_{J,:} \in R^{|J|\times d}$
  \item [] Note that we only need space for storing one of them. It can be overwritten once used.
  \item $\mQ_{I,:}   \mK_{J,:}^\top \in R^{|I|\times |J|}$
  \end{enumerate}
\item We assume that \eqref{eq:updateO} does not require
  extra space to store the intermediate result
  $(\mQ_{I_i,:}   \mK_{J_j,:}^\top)  \mV_{J_j,:}$. This is possible
  as any results can be immediately used to update $O$.
\item Therefore, we select $|I|$ and $|J|$ to satisfy
  \begin{equation}
    \label{eq:IJcond}
    |I| d < \frac{M}{4}, \quad |J|d < \frac{M}{4}, \quad |I||J| < \frac{M}{4}.
  \end{equation}
\item We also need that
  \begin{center}
    $|I|$ as large as possible,
  \end{center}
  so $K, V$ are less frequently loaded.
The reasons is that for any $I$, we must access the whole $K, V$

\item If we choose
  \begin{equation*}
    |I| = \left\lfloor \frac{M}{4d} \right\rfloor \text{ and }
    |J| = \min (\left\lfloor{\frac{M}{4d}}\right\rfloor, d)
  \end{equation*}
  then \eqref{eq:IJcond} holds.
\item In the end, the number of memory accesses is
  \begin{equation}
    \label{eq:final_complexity}
    O(\frac{T}{M/d}) \times O(Td) = O(\frac{T^2d^2}{M}).
  \end{equation}
\item This value is smaller than the one we had earlier
  in \eqref{eq:Ncubic}.
\item Unfortunately, we need the whole intermediate matrix
  in \eqref{eq:intermediate}
  because the SoftMax function involves all elements in each row.
\item A crucial observation is that we hope to have
       \framebreak
  \begin{equation}
    \hspace{-1.5cm}
        \label{eq:recuriveupdate}
    \begin{split}
      & \text{SoftMax} (
      \begin{bmatrix}
        \mQ_{I,:}\mK_{J_1,:}^\top & \cdots & \mQ_{I,:}\mK_{J_{j},:}^\top
      \end{bmatrix}
      )
      \begin{bmatrix}
        \mV_{J_1,:} \\ \vdots \\ \mV_{J_{j},:}
      \end{bmatrix} =
      \\
      &
      \Delta_{j-1} \odot
      (\text{SoftMax} (
        \begin{bmatrix}
          \mQ_{I,:}\mK_{J_1,:}^\top & \cdots & \mQ_{I,:}\mK_{J_{j-1},:}^\top
        \end{bmatrix}
        )
        \begin{bmatrix}
          \mV_{J_1,:} \\ \vdots \\ \mV_{J_{j-1},:}
        \end{bmatrix}) 
      \\
      & +
      \Delta_{j} \odot
      \left(\text{SoftMax} (
        \mQ_{I,:}\mK_{J_{j},:}^\top
        ) \mV_{J_{j},:}\right),
    \end{split}
        \end{equation}
  where  $\odot$ is the
  component-wise product and $\Delta_{j-1}, \Delta_{j} \in R^{|I|}$.
  If $\Delta_{j-1}, \Delta_{j}$ are easily
  available, then we can do an operation similar to \eqref{eq:updateO}.
\item We discuss why $\Delta_j$ can be easily calculated. While
  $\Delta_j$ is a vector, we give an illustration to get
  one of its components. 
\item We have
$ \forall t \in J_1 \cup \cdots \cup J_{j-1}$,
  \begin{equation*}
    \begin{split}
&      \frac{\exp{(z_t)}}{\sum_{s \in J_1 \cup \cdots \cup J_{j}}
  \exp{(z_s)}}
\\
= &
     \underbrace{\left(\frac{\sum_{s \in J_1 \cup \cdots \cup J_{j-1}}\exp{(z_s)}}{\sum_{s \in J_1 \cup \cdots \cup J_{j}}
       \exp{(z_s)}}\right)}_{\Delta_{j-1}}
     \frac{\exp{(z_t)}}{\sum_{s \in J_1 \cup \cdots \cup J_{j-1}}
       \exp{(z_s)}}, 
    \end{split}
  \end{equation*}
  and
    $\forall t \in J_{j}$,
  \begin{equation*}
    \begin{split}
&      \frac{\exp{(z_t)}}{\sum_{s \in J_1 \cup \cdots \cup J_{j}}
  \exp{(z_s)}}
\\
= &
     \underbrace{\left(\frac{\sum_{s \in J_{j}}\exp{(z_s)}}{\sum_{s \in J_1 \cup \cdots \cup J_{j}}
       \exp{(z_s)}}\right)}_{\Delta_{j}}
     \frac{\exp{(z_t)}}{\sum_{s \in J_{j}}
       \exp{(z_s)}}.
    \end{split}
  \end{equation*}
\item Clearly, all we need is to maintain
  \begin{equation}
\label{eq:expsum}    
\sum_{s \in J_1 \cup \cdots \cup J_{j}}
       \exp{(z_s)}.    
  \end{equation}
\item When handling $j$, we get
  \begin{equation*}
    \sum_{s \in J_{j}}
       \exp{(z_s)},
     \end{equation*}
     so we can update \eqref{eq:expsum}
     by
     \begin{equation*}
       \begin{split}
         &
           \sum_{s \in J_1 \cup \cdots \cup J_{j}}
           \exp{(z_s)}\\
         = &
           \sum_{s \in J_1 \cup \cdots \cup J_{j-1}}
           \exp{(z_s)} + 
    \sum_{s \in J_{j}}
       \exp{(z_s)}.
       \end{split}
     \end{equation*}
   \item This $O(|I|)$ cost for storing \eqref{eq:expsum}
     in main memory is affordable.
   \item To have an algorithm, let's define
   \begin{eqnarray*}
       &&       \bar{\Delta}_{\text{old}}
          = \exp(
      \begin{bmatrix}
        \mQ_{I,:}\mK_{J_1,:}^\top & \cdots & \mQ_{I,:}\mK_{J_{j-1},:}^\top
      \end{bmatrix}
)\text{'s row sum}                                             
\\
       &&       \bar{\Delta}
          = \exp(
      \begin{bmatrix}
        \mQ_{I,:}\mK_{J_1,:}^\top & \cdots & \mQ_{I,:}\mK_{J_{j},:}^\top
      \end{bmatrix}
)\text{'s row sum}                                             
       \\
&&       \hat{\Delta} = \exp(
\mQ_{I,:}\mK_{J_{j},:}^\top)\text{'s row sum}                        
     \end{eqnarray*}
   \item Then \eqref{eq:recuriveupdate} becomes
     \begin{equation*}
       \hspace{-1cm}
       \begin{split}
&      \frac{\bar{\Delta}_{\text{old}}}{\bar{\Delta}} \odot
      (\text{SoftMax} (
        \begin{bmatrix}
          \mQ_{I,:}\mK_{J_1,:}^\top & \cdots & \mQ_{I,:}\mK_{J_{j-1},:}^\top
        \end{bmatrix}
        )
        \begin{bmatrix}
          \mV_{J_1,:} \\ \vdots \\ \mV_{J_{j-1},:}
        \end{bmatrix}) 
      \\
      & +
      \frac{\hat{\Delta}}{\bar{\Delta}} \odot
      \left(\text{SoftMax} (
        \mQ_{I,:}\mK_{J_{j},:}^\top
        ) \mV_{J_{j},:}\right),
      \\
      = &
      \frac{\bar{\Delta}_{\text{old}}}{\bar{\Delta}} \odot
      (\text{SoftMax} (
        \begin{bmatrix}
          \mQ_{I,:}\mK_{J_1,:}^\top & \cdots & \mQ_{I,:}\mK_{J_{j-1},:}^\top
        \end{bmatrix}
        )
        \begin{bmatrix}
          \mV_{J_1,:} \\ \vdots \\ \mV_{J_{j-1},:}
        \end{bmatrix}) 
      \\
      & +
      \frac{1}{\bar{\Delta}} \odot
\left(\exp(\mQ_{I,:}\mK_{J_{j},:}^\top)
        \mV_{J_{j},:}\right).
       \end{split}
     \end{equation*}
   \item Finally, we have the following algorithm.
     \framebreak
 \item For $i = 1, \ldots, T_r$
   \begin{itemize}
   \item Load  $\mQ_{I_i,:}$
   \item For $j = 1, \ldots, T_c$
     \begin{eqnarray}
       &&       \bar{\Delta}_{\text{old}} =  \bar{\Delta} \nonumber\\
       && \text{Load } \mK_{J_j,:} \text{ and } P= \exp(\mQ_{I_i,:}   \mK_{J_j,:}^\top)  \label{eq:exp} \\
       &&       \bar{\Delta} \leftarrow \bar{\Delta} + P\text{'s row sum} \nonumber \\
       && \text{Load } \mV_{J_j,:} \text{ to overwrite } \mK_{J_j,:}
          \nonumber \\
 &&       \label{eq:updateO1}
       O_{I_i,:} = \frac{\bar{\Delta}_{\text{old}}}{\bar{\Delta}} \odot O_{I_i,:} +  \frac{1}{\bar{\Delta}} \odot (P\mV_{J_j,:})
     \end{eqnarray}
   \item Save $O_{I_i,:}$ to secondary memory.
   \end{itemize}
        \framebreak
 \item In \eqref{eq:updateO1}, the fraction means component-wise
   division.
 \item For \eqref{eq:exp}, we can calculate
   $\mQ_{I_i,:}   \mK_{J_j,:}^\top$ first and calculate the exp value
   of each component. Thus, we only need space for one matrix.
   
 \item The number of memory accesses is the same as the algorithm
   analyzed in \eqref{eq:updateO} and \eqref{eq:final_complexity},
   \begin{equation*}
   	O(\frac{T^2 d^2}{M}).
   \end{equation*}
%    \item To apply SoftMax, the intermediate results, $\mQ_{I,:} \mK_{I_s,:}^\top \in R^{|I| \times |I|}, \forall s$, must be stored in the main memory first.
%    \item This requires that an $|I|\times |I|$ matrix fit into memory, i.e.,
%    \begin{equation}
%    		|I| \times |I| = O(M).
%    \label{eq:Isize_cond}
%    \end{equation}
%    \item Since $|I| = O(\frac{M}{d})$, the condition~\eqref{eq:Isize_cond} becomes
%    \begin{equation}
%    		O(\frac{M^2}{d^2}) = O(M).
%    \label{eq:Isize_cond2}
%    \end{equation}
%    \item To satisfy \eqref{eq:Isize_cond2}, one solution is to adopt different block sizes, $|I^{\text{row}}|$ for $\mQ$ and $|I^{\text{col}}|$ for $\mK, \mV$, where
%    \begin{equation*}
%    		|I^{\text{row}}| = O(\min(\frac{M}{d}, d)),~\text{while}~|I^{\text{col}}| = O(\frac{M}{d}).
%    \end{equation*}
%    \item With this scheme, $Q,K,V$ are partitioned into row blocks:
%   \begin{equation*}
%     \begin{bmatrix}
%       Q_{I^{\text{row}}_1,:}\\
%       \vdots\\
%       Q_{I^{\text{row}}_{\bar{T}},:}
%     \end{bmatrix},
%     \begin{bmatrix}
%       K_{I^{\text{col}}_1,:}\\
%       \vdots\\
%       K_{I^{\text{col}}_{\tilde{T}},:}
%     \end{bmatrix},\text{and}
%     \begin{bmatrix}
%       V_{I^{\text{col}}_1,:}\\
%       \vdots\\
%       V_{I^{\text{col}}_{\tilde{T}},:}
%     \end{bmatrix},
%   \end{equation*}
% where
%   \begin{equation*}
%     |I^{\text{row}}_1| = \cdots = |I^{\text{row}}_{\bar{T}}| = O(\min(\frac{M}{d}, d)),
%   \end{equation*}
%   and 
%   \begin{equation*}
%     |I^{\text{col}}_1| = \cdots = |I^{\text{col}}_{\tilde{T}}| = O(\frac{M}{d}).
%   \end{equation*}
%    \item {\bf At this point, we need to consider the order of the two nested for-loops: should the iteration proceed over rows before columns, or vice versa?}
   \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{FlashAttention and FlashAttention-2}
\begin{itemize}
\item FlashAttention was first introduced in \cite{TD22a}.
\item Later \cite{TD24a} proposed an improved version FlashAttention-2.
\item Interestingly, what we described earlier is FlashAttention-2.
\item In the original FlashAttention, the algorithm is as follows
  \framebreak
   \item For $j = 1, \ldots, T_c$  
     \begin{itemize}
     \item Load  $\mV_{J_j,:}$ and  $\mK_{J_j,:}$
 \item For $i = 1, \ldots, T_r$
 \begin{eqnarray*}
   && \text{Load } \bar{\Delta}_{I_i}, \mQ_{I_i,:} \text{ and } P= \exp(\mQ_{I_i,:}   \mK_{J_j,:}^\top) \\
      &&       \bar{\Delta}_{\text{old}} =  \bar{\Delta}_{I_i} \\
      &&       \bar{\Delta}_{I_i} \leftarrow \bar{\Delta}_{I_i} + P\text{'s row sum} \nonumber \\
       && \text{Load } \mO_{I_i,:} \text{ to overwrite } \mQ_{I_i,:}
\\
 &&      
    \mO_{I_i,:} = \frac{\bar{\Delta}_{\text{old}}}{\bar{\Delta}} \odot \mO_{I_i,:} +  \frac{1}{\bar{\Delta}} \odot (P\mV_{J_j,:})
       \\
      && \text{Save }   \mO_{I_i,:}, \bar{\Delta}_{I_i}  \text{ to secondary memory} 
    \end{eqnarray*}
   \end{itemize}
        \framebreak
      \item A comparison with FlashAttention-2 shows that here $i$ and $j$ are swapped.
      \item Like FlashAttention-2, FlashAttention also maintain four matrices in the
        main memory
        \begin{enumerate}
  \item $K_{J,:} \in R^{|J|\times d}$
  \item $V_{J,:} \in R^{|J|\times d}$
          
\item $Q_{I,:} \in R^{|I|\times d}$ \text{ or }  $O_{I,:} \in R^{|I|\times d}$
  \item [] Note that we only need space for storing one of them. It can be overwritten once used.
  \item $\mQ_{I,:}   \mK_{J,:}^\top \in R^{|I|\times |J|}$
  \end{enumerate}
\item In the inner loop, we now need to
  \begin{itemize}
  \item load two matrices, and
  \item save one matrix and one vector,
  \end{itemize}
  from/to secondary memory, respectively.
\item However, the earlier version (i.e., FlashAttention-2)
  needs to \alert{load only two matrices}.
\item The main issue is that FlashAttention sequentially uses
  \begin{equation*}
    \mO_{I_i,:}, \forall i
  \end{equation*}
  in the inner loop, but we cannot afford to store them.
  Thus, we must save $\mO_{I_i,:}$ back to secondary memory.
\item Another issue is we must ensure that the save
  operation of $\mO_{I_i,:}$ is finished before the load
  operation in iteration $i+1$. The reason is to free space
  for loading $\mQ_{I_{i+1},:}$ or $P$.
\item The above discussion explains why FlashAttention-2 is more
  memory efficient than FlashAttention.
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{Practical Implementation}
\begin{itemize}
%  \item The above computations and the update of $l$ continue until all iterations are completed.
  \item The algorithm described above appear sufficient to perform the complete attention computation.
  \item Nevertheless, in practical implementations, the SoftMax function is computed in a modified form to ensure numerical stability.
  \item Specifically, instead of directly computing
  \begin{equation*}
  	\frac{\exp{(z_t)}}{\sum_{s=1}^{T} \exp{(z_s)}},
  \end{equation*}
  the modified form below is commonly used,
  \begin{equation*}
  	\frac{\exp{(z_t - m(z))}}{\sum_{s=1}^{T} \exp{(z_s-m(z))}},
  \end{equation*}
  where
  \begin{equation*}
  	m(z) = \max_{1\leq s \leq T}~z_s.
  \end{equation*}
	\item Then,
	$ \forall t \in J_1 \cup \cdots \cup J_{j-1}$,
	  \begin{equation*}
	    \begin{split}
	& \frac{\exp{(z_t)}}{\sum_{s \in J_1 \cup \cdots \cup J_{j}}
	  \exp{(z_s)}}
	\\
	= & \frac{\exp{(z_t - m(z))}}{\sum_{s \in J_1 \cup \cdots \cup J_{j}}
	  \exp{(z_s - m(z))}}
	    \end{split}
	  \end{equation*}
	  and
	    $\forall t \in J_{j}$,
	  \begin{equation*}
	    \begin{split}
	& \frac{\exp{(z_t)}}{\sum_{s \in J_1 \cup \cdots \cup J_{j}}
	  \exp{(z_s)}}
	\\
	= & \frac{\exp{(z_t - m(z))}}{\sum_{s \in J_1 \cup \cdots \cup J_{j}}
	  \exp{(z_s - m(z))}}
	    \end{split}
	  \end{equation*}
  \item Since $m(z)$ requires all elements in each row, it introduces a new challenge for  our block-wise algorithm.
  \item Fortunately, a crucial observation is that $m(z)$ is decomposable by block, as the maximum operation is associative.
  \item Specifically,
  \begin{equation*}
  \begin{split}
  	m(z) = &\max_{1\leq s \leq T}~z_s, \\
  	= &\max_{s \in J_1 \cup \cdots \cup J_{T_c}}~z_s, \\
  	= &\max~\{\max_{s \in J_1}~z_s,~\cdots,~\max_{s \in J_{T_c}}~z_s\}.
  \end{split}
  \end{equation*}
  \item Therefore, we have 
  $ \forall t \in J_1 \cup \cdots \cup J_{j-1}$,
  \begin{align*}
  	&\exp{(z_t - \max_{s \in J_1 \cup \cdots \cup J_j}~z_s)} \\
  	= &\exp{(z_t - \max_{s \in J_1 \cup \cdots \cup J_{j-1}}~z_s)}\\
  	\times &\exp{(\max_{s \in J_1 \cup \cdots \cup J_{j-1}}~z_s - \max_{s \in J_1 \cup \cdots \cup J_{j}}~z_s)}\\
  	= & \exp{(z_t - \max_{s \in J_1 \cup \cdots \cup J_{j-1}}~z_s)} \\
  	\times &\exp{(\max_{s \in J_1 \cup \cdots \cup J_{j-1}}z_s - \max \{\max_{s \in J_j}~z_s, \max_{s \in J_1 \cup \cdots \cup J_{j-1}}z_s\})}
  \end{align*}
  and $ \forall t \in J_{j}$,
  \begin{align*}
  	&\exp{(z_t - \max_{s \in J_1 \cup \cdots \cup J_j}~z_s)} \\
  	= &\exp{(z_t - \max_{s \in J_j}~z_s)}\\
  	&\times \exp{(\max_{s \in J_j}~z_s - \max_{s \in J_1 \cup \cdots \cup J_{j}}~z_s)}\\
  	= & \exp{(z_t - \max_{s \in J_j}~z_s)} \\
  	&\times\exp{(\max_{s \in J_j}~z_s - \max \{\max_{s \in J_j}~z_s, \max_{s \in J_1 \cup \cdots \cup J_{j-1}}~z_s\})}
  \end{align*}
  \item Like the sum of exponents in \eqref{eq:expsum}, all we need is to maintain
  \begin{equation}
  	\max_{s \in J_1 \cup \cdots \cup J_{j}}~z_s.
  \label{eq:logitmax}
  \end{equation}
  \item When handling $j$, we get
  \begin{equation*}
  	\max_{s \in J_j}~z_s,
  \end{equation*}
  so we can update \eqref{eq:logitmax} by 
  \begin{align*}
  	&\max_{s \in J_1 \cup \cdots \cup J_{j}}~z_s \\
  	= &\max \{\max_{s \in J_j}~z_s, \max_{s \in J_1 \cup \cdots \cup J_{j-1}}~z_s\}.
  \end{align*}
  \item Like \eqref{eq:expsum}, the cost for storing \eqref{eq:logitmax} is also $O(|I|)$ and affordable.
	\item By integrating the maintenance of \eqref{eq:logitmax} into our original algorithm, we can obtain a new algorithm that ensures numerical stability in practice.
\end{itemize}
\end{frame}

\section{Standard Attention Backward Pass}

\begin{frame}[allowframebreaks]
\frametitle{Motivation for Backward Pass Analysis}
\begin{itemize}
\item Attention mechanisms are fundamental to transformers and modern LLMs, making efficient training critical for practical applications.
\item Memory-optimized attention implementations often avoid storing large intermediate matrices that standard gradient computation requires, necessitating manual gradient derivation.
\item Understanding the standard backward pass reveals memory and computational bottlenecks that motivate the development of more efficient attention algorithms.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Backward Pass: Problem Setup}
\begin{itemize}
\item We consider the standard attention forward pass computation, which consists of three sequential operations.
\item Given input matrices $\mQ, \mK, \mV \in R^{T \times d}$, where $T$ denotes the sequence length and $d$ denotes the head dimension, the forward pass computes:
  \begin{align}
    \mS &= \mQ \mK^ \top \in R^{T \times T} \label{eq:s_eq}\\
    \mP &= \text{SoftMax}(\mS) \in R^{T \times T} \label{eq:softmax_eq}\\
    \mO &= \mP \mV \in R^{T \times d} \label{eq:output_eq}
  \end{align}
  where $\mS$ contains the attention scores, $\mP$ represents the attention probabilities, and $\mO$ is the final output.
\item We assume that the gradient with respect to the output
  \begin{equation*}
    \frac{\partial L}{\partial \mO} \in R^{T \times d}
  \end{equation*}
  is available from subsequent layers in the neural network, where $L$ is the scalar loss function.
\item Our objective is to compute the gradients with respect to the three input matrices:
  \begin{equation*}
    \frac{\partial L}{\partial \mQ}, \quad \frac{\partial L}{\partial \mK}, \quad \text{and} \quad \frac{\partial L}{\partial \mV}
  \end{equation*}
\item We derive these gradients by applying the chain rule through the computational graph of the attention mechanism. For illustration purposes, we use simplified dimensions $T=2$ and $d=1$ in our computational graph examples.
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{Gradient w.r.t. V: ($\partial L / \partial \mV$)}
\begin{itemize}
\item From \eqref{eq:output_eq}, we have 
\begin{equation*}
  O_{ik} = \sum_{j=1}^T P_{ij} V_{jk}
\end{equation*}
which defines each output element, where $i$ = $1, \ldots, T$ and $k$ = $1, \ldots, d$.
\item Each element $V_{jk}$ affects multiple outputs $O_{ik}$ for all positions $i$, as shown in the computational graph below.
\end{itemize}

\begin{center}
\begin{tikzpicture}[
  auto,
  intermediate/.style={draw,circle,minimum width=0.8cm,thick,font=\small},
  every label/.append style={font=\scriptsize}
]
  % Input labels (text only)
  \node[font=\small] (x1) at (0,3) {$P_{11}$};
  \node[font=\small] (x2) at (0,2) {$P_{12}$};
  \node[font=\small] (x3) at (0,1) {$P_{21}$};
  \node[font=\small] (x4) at (0,0) {$P_{22}$};
  \node[font=\small] (x5) at (0,-1) {$V_{11}$};
  \node[font=\small] (x6) at (0,-2) {$V_{21}$};

  % Intermediate nodes (v)
  \node[intermediate] (v1) at (3,2.5) {$v_1$};
  \node[intermediate] (v2) at (3,1.5) {$v_2$};
  \node[intermediate] (v3) at (3,0.5) {$v_3$};
  \node[intermediate] (v4) at (3,-0.5) {$v_4$};

  % Output nodes
  \node[intermediate] (v5) at (6,1.5) {$v_5$};
  \node[intermediate] (v6) at (6,0) {$v_6$};

  % Loss function
  \node[font=\small] (L) at (8.5,0.75) {$L$};

  % Forward edges
  \path[-{Stealth[scale=0.8]}]
    (x1) edge (v1)
    (x5) edge (v1)
    (x2) edge (v2)
    (x6) edge (v2)
    (x3) edge (v3)
    (x5) edge (v3)
    (x4) edge (v4)
    (x6) edge (v4)
    (v1) edge (v5)
    (v2) edge (v5)
    (v3) edge (v6)
    (v4) edge (v6)
    (v5) edge (L)
    (v6) edge (L);

\end{tikzpicture}
\end{center}
\begin{itemize}
\item The intermediate nodes in the computational graph represent element-wise products that we define as follows:
  \begin{align*}
    v_1 &= P_{11} \times V_{11} \\
    v_2 &= P_{12} \times V_{21} \\
    v_3 &= P_{21} \times V_{11} \\
    v_4 &= P_{22} \times V_{21} \\
    v_5 &= v_1 + v_2 = P_{11}V_{11} + P_{12}V_{21} = O_{11} \\
    v_6 &= v_3 + v_4 = P_{21}V_{11} + P_{22}V_{21} = O_{21}
  \end{align*}

\end{itemize}
\begin{itemize}
\item The element $V_{11}$ affects the loss $L$ through both $v_1$ and $v_3$, and we can compute its gradient using the chain rule:
\begin{align*}
\frac{\partial L}{\partial V_{11}} &= \frac{\partial L}{\partial v_5} \frac{\partial v_5}{\partial v_1} \frac{\partial v_1}{\partial V_{11}} + \frac{\partial L}{\partial v_6} \frac{\partial v_6}{\partial v_3} \frac{\partial v_3}{\partial V_{11}} \\[0.2cm]
&= \frac{\partial L}{\partial O_{11}} \cdot 1 \cdot P_{11} + \frac{\partial L}{\partial O_{21}} \cdot 1 \cdot P_{21} \\[0.2cm]
&= \sum_{i=1}^T P_{i1} \frac{\partial L}{\partial O_{i1}}
\end{align*}
\end{itemize}
\vspace{0.5cm}
\begin{itemize}
\item To generalize this approach, consider an arbitrary element $V_{jk}$ of the value matrix $\mV$.
\item The element $V_{jk}$ contributes to the computation of output $O_{ik}$ for all positions $i = 1, 2, \ldots, T$ through the matrix multiplication $\mO = \mP \mV$.
\item Specifically, each output element is computed as:
  \begin{equation*}
    O_{ik} = \sum_{j'=1}^T P_{ij'} V_{j'k}
  \end{equation*}
\item The element $V_{jk}$ appears in this sum when $j' = j$, contributing the term $P_{ij} V_{jk}$ to $O_{ik}$.
\end{itemize}
\vspace{0.5cm}
\begin{itemize}
\item Using the chain rule to compute the gradient of $V_{jk}$:
\begin{align*}
\frac{\partial L}{\partial V_{jk}} &= \sum_{i=1}^T \frac{\partial L}{\partial O_{ik}} \frac{\partial O_{ik}}{\partial V_{jk}} \\[0.2cm]
&= \sum_{i=1}^T \frac{\partial L}{\partial O_{ik}} \frac{\partial}{\partial V_{jk}} \left( \sum_{j'=1}^T P_{ij'} V_{j'k} \right) \\[0.2cm]
&= \sum_{i=1}^T \frac{\partial L}{\partial O_{ik}} P_{ij}
\end{align*}
\item This gives us the general formula for any element of $V$:
  \begin{equation*}
    \frac{\partial L}{\partial V_{jk}} = \sum_{i=1}^T \frac{\partial L}{\partial O_{ik}} P_{ij}
  \end{equation*}

\item This expression corresponds exactly to the $(j,k)$-th element of the matrix product:
  \begin{equation*}
    \left(\mP^\top \frac{\partial L}{\partial \mO}\right)_{jk} = \sum_{i=1}^T P_{ij} \frac{\partial L}{\partial O_{ik}}
  \end{equation*}

\item Therefore, we can express the complete gradient with respect to $\mV$ as:
  \begin{equation*}
    \frac{\partial L}{\partial \mV} = \mP^\top \frac{\partial L}{\partial \mO} \in R^{T \times d}
  \end{equation*}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Gradient w.r.t. P ($\partial L / \partial \mP$)}
\begin{itemize}
\item We now derive the gradient with respect to the attention weights matrix $\mP \in R^{T \times T}$.
\item Using the chain rule, we compute the gradient for an individual element $P_{ij}$:
  \begin{equation*}
    \frac{\partial L}{\partial P_{ij}} = \sum_{k=1}^d \frac{\partial L}{\partial O_{ik}} \frac{\partial O_{ik}}{\partial P_{ij}}
  \end{equation*}
\item Since:
  \begin{equation*}
    O_{ik} = \sum_{j'=1}^T P_{ij'} V_{j'k}
  \end{equation*}
  we have the partial derivative:
  \begin{equation*}
    \frac{\partial O_{ik}}{\partial P_{ij}} = V_{jk}
  \end{equation*}
\item Substituting this result gives us:
  \begin{equation*}
    \frac{\partial L}{\partial P_{ij}} = \sum_{k=1}^d \frac{\partial L}{\partial O_{ik}} V_{jk}
  \end{equation*}
\item To express this in matrix form, we observe that the sum over $k$ represents the $j$-th row of $\mV$ multiplied with the $i$-th row of 
\begin{equation*}
  \frac{\partial L}{\partial \mO} \in R^{T \times d}
\end{equation*}
\item Therefore, the complete gradient matrix is:
  \begin{equation*}
    \frac{\partial L}{\partial \mP} = \frac{\partial L}{\partial \mO} \mV^\top \in R^{T \times T}
  \end{equation*}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Gradient w.r.t. S ($\partial L / \partial \mS$)}
\begin{itemize}
\item We derive the gradient with respect to the attention scores matrix $\mS \in R^{T \times T}$.
\item Given:
\begin{equation*}
  \frac{\partial L}{\partial \mP} \in R^{T \times T}
\end{equation*}
we need to propagate this gradient through the SoftMax operation in \eqref{eq:softmax_eq}.
\item The computational graph below illustrates the SoftMax computation for a $2 \times 2$ case.
\end{itemize}


\begin{center}
\begin{tikzpicture}[
  auto,
  intermediate/.style={draw,circle,minimum width=0.8cm,thick,font=\small},
  every label/.append style={font=\scriptsize}
]
  % Input labels (text only)
  \node[font=\small] (s1) at (0,3) {$S_{11}$};
  \node[font=\small] (s2) at (0,2) {$S_{12}$};
  \node[font=\small] (s3) at (0,1) {$S_{21}$};
  \node[font=\small] (s4) at (0,0) {$S_{22}$};

  % Intermediate nodes (exponentials)
  \node[intermediate] (v1) at (3,3) {$v_1$};
  \node[intermediate] (v2) at (3,2) {$v_2$};
  \node[intermediate] (v3) at (3,1) {$v_3$};
  \node[intermediate] (v4) at (3,0) {$v_4$};

  % Row sum nodes
  \node[intermediate] (v5) at (6,2.5) {$v_5$};
  \node[intermediate] (v6) at (6,0.5) {$v_6$};

  % Output nodes (probabilities)
  \node[font=\small] (p1) at (9,3) {$P_{11}$};
  \node[font=\small] (p2) at (9,2) {$P_{12}$};
  \node[font=\small] (p3) at (9,1) {$P_{21}$};
  \node[font=\small] (p4) at (9,0) {$P_{22}$};

  % Forward edges
  \path[-{Stealth[scale=0.8]}]
    (s1) edge (v1)
    (s2) edge (v2)
    (s3) edge (v3)
    (s4) edge (v4)
    (v1) edge (v5)
    (v2) edge (v5)
    (v3) edge (v6)
    (v4) edge (v6)
    (v1) edge[bend left=20] (p1)
    (v5) edge (p1)
    (v2) edge[bend right=20] (p2)
    (v5) edge (p2)
    (v3) edge[bend left=20] (p3)
    (v6) edge (p3)
    (v4) edge[bend right=20] (p4)
    (v6) edge (p4);

\end{tikzpicture}
\end{center}

\begin{itemize}
\item The nodes in the computational graph are defined as follows:
  \begin{align*}
    v_1 &= e^{S_{11}}, \quad v_2 = e^{S_{12}}, \quad v_3 = e^{S_{21}}, \quad v_4 = e^{S_{22}} \\
    v_5 &= v_1 + v_2 = e^{S_{11}} + e^{S_{12}} \\
    v_6 &= v_3 + v_4 = e^{S_{21}} + e^{S_{22}} \\
    P_{11} &= \frac{v_1}{v_5}, \quad P_{12} = \frac{v_2}{v_5}, \quad P_{21} = \frac{v_3}{v_6}, \quad P_{22} = \frac{v_4}{v_6}
  \end{align*}
\item Each row of the attention scores matrix is independently normalized to form the corresponding row of the probability matrix $\mP$.
\end{itemize}

\begin{itemize}
\item From the computational graph, we observe that $S_{11}$ affects the loss $L$ through both $P_{11}$ and $P_{12}$:
  \begin{equation*}
    \frac{\partial L}{\partial S_{11}} = \frac{\partial L}{\partial P_{11}} \frac{\partial P_{11}}{\partial S_{11}} + \frac{\partial L}{\partial P_{12}} \frac{\partial P_{12}}{\partial S_{11}}
  \end{equation*}
\item We compute the required partial derivatives:
  \begin{align*}
    \frac{\partial P_{11}}{\partial S_{11}} &= \frac{\partial}{\partial S_{11}}\left(\frac{v_1}{v_5}\right) = \frac{v_5 \frac{\partial v_1}{\partial S_{11}} - v_1 \frac{\partial v_5}{\partial S_{11}}}{v_5^2} \\
    &= \frac{v_5 \cdot e^{S_{11}} - v_1 \cdot e^{S_{11}}}{v_5^2} = \frac{e^{S_{11}}(v_5 - v_1)}{v_5^2} \\
    &= \frac{e^{S_{11}} \cdot v_2}{v_5^2} = \frac{v_1 \cdot v_2}{v_5^2} = P_{11}(1 - P_{11})
  \end{align*}
  \begin{align*}
    \frac{\partial P_{12}}{\partial S_{11}} &= \frac{\partial}{\partial S_{11}}\left(\frac{v_2}{v_5}\right) = \frac{v_5 \frac{\partial v_2}{\partial S_{11}} - v_2 \frac{\partial v_5}{\partial S_{11}}}{v_5^2} \\
    &= \frac{v_5 \cdot 0 - v_2 \cdot e^{S_{11}}}{v_5^2} = \frac{-v_2 \cdot v_1}{v_5^2} = -P_{12} P_{11}
  \end{align*}
\end{itemize}

\begin{itemize}
\item Substituting back:
  \begin{align*}
    \frac{\partial L}{\partial S_{11}} &= \frac{\partial L}{\partial P_{11}} \cdot P_{11}(1 - P_{11}) + \frac{\partial L}{\partial P_{12}} \cdot (-P_{12} P_{11}) \\
    &= P_{11} \left( \frac{\partial L}{\partial P_{11}} - \sum_{k=1}^2 P_{1k}\frac{\partial L}{\partial P_{1k}} \right)
  \end{align*}
\item To generalize this result, we consider an arbitrary element $S_{ij}$ in a $T \times T$ attention scores matrix.
\item Following the same chain rule approach, $S_{ij}$ affects the loss $L$ through all probability elements in row $i$:
  \begin{equation*}
    \frac{\partial L}{\partial S_{ij}} = \sum_{k=1}^T \frac{\partial L}{\partial P_{ik}} \frac{\partial P_{ik}}{\partial S_{ij}}
  \end{equation*}
\item To compute the partial derivatives 
\begin{equation*}
  \frac{\partial P_{ik}}{\partial S_{ij}}
\end{equation*}
we apply the quotient rule to the softmax expression:
  \begin{equation*}
    P_{ik} = \frac{e^{S_{ik}}}{\sum_{\ell=1}^T e^{S_{i\ell}}}
  \end{equation*}
  This yields two cases depending on whether the indices match:
  \begin{align*}
    \frac{\partial P_{ik}}{\partial S_{ij}} &= \begin{cases}
      P_{ik}(1 - P_{ik}) & \text{if } k = j \\
      -P_{ik} P_{ij} & \text{if } k \neq j
    \end{cases}
  \end{align*}
\item Substituting these derivatives into the chain rule expression:
  \begin{align*}
    \frac{\partial L}{\partial S_{ij}} &= \frac{\partial L}{\partial P_{ij}} P_{ij}(1 - P_{ij}) + \sum_{k \neq j} \frac{\partial L}{\partial P_{ik}} (-P_{ik} P_{ij}) \\
    &= P_{ij} \left( \frac{\partial L}{\partial P_{ij}} - \sum_{k=1}^T P_{ik}\frac{\partial L}{\partial P_{ik}} \right)
  \end{align*}
\item This gives us the general SoftMax gradient formula:
  \begin{equation}
    \frac{\partial L}{\partial S_{ij}} = P_{ij} \left( \frac{\partial L}{\partial P_{ij}} - \sum_{k=1}^T P_{ik}\frac{\partial L}{\partial P_{ik}} \right)
    \label{eq:softmax_gradient}
  \end{equation}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Gradient w.r.t. K ($\partial L / \partial \mK$)}
\begin{itemize}
\item From \eqref{eq:s_eq}, we have the relation:
\begin{equation*}
S_{ij} = \sum_{\ell=1}^d Q_{i\ell} K_{j\ell}
\end{equation*}
\item The computational graph demonstrates how Q and K matrix elements combine through element-wise products to generate attention scores S.
\end{itemize}

\begin{center}
\begin{tikzpicture}[
  auto,
  intermediate/.style={draw,circle,minimum width=0.8cm,thick,font=\small},
  every label/.append style={font=\scriptsize}
]
  % Input labels (text only)
  \node[font=\small] (q1) at (0,2.5) {$Q_{11}$};
  \node[font=\small] (q2) at (0,1.5) {$Q_{21}$};
  \node[font=\small] (k1) at (0,0.5) {$K_{11}$};
  \node[font=\small] (k2) at (0,-0.5) {$K_{21}$};

  % Intermediate nodes (products)
  \node[intermediate] (v1) at (3,2.5) {$v_1$};
  \node[intermediate] (v2) at (3,1.5) {$v_2$};
  \node[intermediate] (v3) at (3,0.5) {$v_3$};
  \node[intermediate] (v4) at (3,-0.5) {$v_4$};

  % Output nodes (attention scores)
  \node[font=\small] (s11) at (6,2.5) {$S_{11}$};
  \node[font=\small] (s12) at (6,1.5) {$S_{12}$};
  \node[font=\small] (s21) at (6,0.5) {$S_{21}$};
  \node[font=\small] (s22) at (6,-0.5) {$S_{22}$};

  % Forward edges
  \path[-{Stealth[scale=0.8]}]
    (q1) edge (v1)
    (k1) edge (v1)
    (q1) edge (v2)
    (k2) edge (v2)
    (q2) edge (v3)
    (k1) edge (v3)
    (q2) edge (v4)
    (k2) edge (v4)
    (v1) edge (s11)
    (v2) edge (s12)
    (v3) edge (s21)
    (v4) edge (s22);

\end{tikzpicture}
\end{center}

\begin{itemize}
\item Node definitions:
  \begin{align*}
    v_1 &= Q_{11} \times K_{11} = S_{11}, \quad v_2 = Q_{11} \times K_{21} = S_{12} \\
    v_3 &= Q_{21} \times K_{11} = S_{21}, \quad v_4 = Q_{21} \times K_{21} = S_{22} \\
  \end{align*}
\end{itemize}

\begin{itemize}
\item From the computational graph, we observe that $K_{11}$ affects the loss $L$ through both $S_{11}$ and $S_{21}$.
\item Using the chain rule, we compute:
  \begin{equation*}
    \frac{\partial L}{\partial K_{11}} = \frac{\partial L}{\partial S_{11}} \frac{\partial S_{11}}{\partial K_{11}} + \frac{\partial L}{\partial S_{21}} \frac{\partial S_{21}}{\partial K_{11}}
  \end{equation*}
\item From our specific $2 \times 2$ example, we have:
\begin{equation*}
S_{11} = Q_{11} K_{11} \quad \text{and} \quad S_{21} = Q_{21} K_{11}
\end{equation*}
\item The partial derivatives are:
  \begin{align*}
    \frac{\partial S_{11}}{\partial K_{11}} &= Q_{11} \\
    \frac{\partial S_{21}}{\partial K_{11}} &= Q_{21}
  \end{align*}
\item Therefore, the gradient with respect to $K_{11}$ is:
  \begin{equation*}
    \frac{\partial L}{\partial K_{11}} = \frac{\partial L}{\partial S_{11}} Q_{11} + \frac{\partial L}{\partial S_{21}} Q_{21} = \sum_{i=1}^2 \frac{\partial L}{\partial S_{i1}} Q_{i1}
  \end{equation*}
\end{itemize}

\begin{itemize}
\item To generalize this approach, consider an arbitrary element $K_{j\ell}$ of the key matrix $\mK$.
\item The element $K_{j\ell}$ contributes to the computation of attention scores $S_{ij}$ for all query positions $i = 1, 2, \ldots, T$ through the matrix multiplication:
\begin{equation*}
S_{ij} = \sum_{\ell'=1}^d Q_{i\ell'} K_{j\ell'}
\end{equation*}
\item The element $K_{j\ell}$ appears in this sum when $\ell' = \ell$, contributing the term $Q_{i\ell} K_{j\ell}$ to $S_{ij}$.
\item Using the chain rule to compute the gradient of $K_{j\ell}$:
\begin{align*}
\frac{\partial L}{\partial K_{j\ell}} &= \sum_{i=1}^T \frac{\partial L}{\partial S_{ij}} \frac{\partial S_{ij}}{\partial K_{j\ell}} \\[0.2cm]
&= \sum_{i=1}^T \frac{\partial L}{\partial S_{ij}} \frac{\partial}{\partial K_{j\ell}} \left( \sum_{\ell'=1}^d Q_{i\ell'} K_{j\ell'} \right) \\[0.2cm]
&= \sum_{i=1}^T \frac{\partial L}{\partial S_{ij}} Q_{i\ell}
\end{align*}
\item This gives us the general formula for any element of $K$:
  \begin{equation*}
    \frac{\partial L}{\partial K_{j\ell}} = \sum_{i=1}^T \frac{\partial L}{\partial S_{ij}} Q_{i\ell}
  \end{equation*}
\item This expression corresponds exactly to the $(j,\ell)$-th element of the matrix product:
  \begin{equation*}
    \left(\left(\frac{\partial L}{\partial \mS}\right)^\top \mQ\right)_{j\ell} = \sum_{i=1}^T \frac{\partial L}{\partial S_{ij}} Q_{i\ell}
  \end{equation*}
\item Therefore, we can express the complete gradient with respect to $\mK$ as:
  \begin{equation*}
    \frac{\partial L}{\partial \mK} = \left(\frac{\partial L}{\partial \mS}\right)^\top \mQ \in R^{T \times d}
  \end{equation*}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Gradient w.r.t. Q: ($\partial L / \partial \mQ$)}
\begin{itemize}
\item Similarly, we can derive the gradient with respect to the query matrix $\mQ$ following the same approach as for $\mK$.
\item For an arbitrary element $Q_{i\ell}$, using the chain rule:
\begin{align*}
\frac{\partial L}{\partial Q_{i\ell}} &= \sum_{j=1}^T \frac{\partial L}{\partial S_{ij}} \frac{\partial S_{ij}}{\partial Q_{i\ell}} = \sum_{j=1}^T \frac{\partial L}{\partial S_{ij}} K_{j\ell}
\end{align*}
\item Therefore, we can express the complete gradient with respect to $\mQ$ as:
  \begin{equation*}
    \frac{\partial L}{\partial \mQ} = \frac{\partial L}{\partial \mS} \mK \in R^{T \times d}
  \end{equation*}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Summary of Gradient Derivations}
\begin{itemize}
\item The attention gradients are derived by applying the chain rule in reverse order of the forward pass computations.
\item Gradients are computed backward from the loss, starting with 
\begin{equation*}
  \frac{\partial L}{\partial \mO}
\end{equation*}
and proceeding through each operation to find the gradients with respect to $\mV, \mP, \mS, \mK,$ and $\mQ$.
\item Key resulting gradients:
  \begin{align}
    \frac{\partial L}{\partial \mV} &= \mP^\top \frac{\partial L}{\partial \mO} \in R^{T \times d}\\
    \frac{\partial L}{\partial \mP} &= \frac{\partial L}{\partial \mO} \mV^\top \in R^{T \times T} \label{eq:gradient_p}\\
    \frac{\partial L}{\partial \mK} &= \left(\frac{\partial L}{\partial \mS}\right)^\top \mQ \in R^{T \times d} \label{eq:gradient_k}\\
    \frac{\partial L}{\partial \mQ} &= \frac{\partial L}{\partial \mS} \mK \in R^{T \times d} \label{eq:gradient_q}
  \end{align}
\framebreak
\item The SoftMax gradient has the form:
  \begin{equation*}
    \frac{\partial L}{\partial S_{ij}} = P_{ij} \odot \left( \frac{\partial L}{\partial P_{ij}} - \sum_{k=1}^T P_{ik}\frac{\partial L}{\partial P_{ik}} \right) \in R^{T \times T} \label{eq:gradient_s}
  \end{equation*}
  where the computation is performed element-wise for each $(i,j)$ position.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{From Mathematical Derivations to Memory-Constrained Implementation}
\begin{itemize}
\item The mathematical gradient formulations provide the foundation for implementing efficient backward pass computation.
\item However, direct implementation encounters significant memory bottlenecks when dealing with large sequence lengths.
\item Consider the memory requirements for intermediate matrices:
\end{itemize}
\begin{equation*}
  \mP, \mS, \frac{\partial L}{\partial \mP}, \frac{\partial L}{\partial \mS} \in R^{T \times T}
\end{equation*}
\begin{itemize}
\item For typical applications where $T \gg d$, storing these $T \times T$ matrices becomes prohibitively expensive.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Block-Based Algorithm Design Strategy}
\begin{itemize}
\item Given the memory constraint:
\begin{equation*}
  d \leq M \leq T \times d
\end{equation*}
where $M$ represents the available fast memory capacity, we cannot compute entire gradient matrices simultaneously.
\item The solution is to employ a block-wise computation strategy that processes gradients in manageable chunks.
\item We assume that matrices $\mP$, $\mS$, $\mO$, and gradient
\begin{equation*}
  \frac{\partial L}{\partial \mO}
\end{equation*}
are stored in secondary memory (HBM).
\item The algorithm computes gradients
\begin{equation*}
  \frac{\partial L}{\partial \mQ}, \quad \frac{\partial L}{\partial \mK}, \quad \text{and} \quad \frac{\partial L}{\partial \mV}
\end{equation*}
by loading only necessary matrix blocks into fast memory at each computational step.
\item This approach ensures memory usage remains within constraint $M$ while maintaining mathematical correctness of the derived gradient formulations.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Block Size Derivation for Standard Attention Backward Pass}
\begin{itemize}
\item To implement a memory-efficient gradient computation, we partition matrices into blocks and process them sequentially.

\item Let $|B_r|$ be the row block size. Our goal is to derive the maximum possible $|B_r|$ that fits within fast memory constraints ($M$).

\framebreak
\item For each gradient in the backward pass, we analyze the peak memory required. This typically involves holding two input blocks and one output block in fast memory simultaneously.
\framebreak
\item The blocks required simultaneously for each gradient are:

\begin{enumerate}
\item Gradient w.r.t. $\mV$:
  \begin{equation*}
    \begin{split}
    &\frac{\partial L}{\partial \mV_{B_r,:}}, \frac{\partial L}{\partial \mO_{B_r,:}} \in R^{|B_r| \times d} \text{ and } \\
    &\mP_{B_r,B_r} \in R^{|B_r| \times |B_r|}
    \end{split}
  \end{equation*}

\item Gradient w.r.t. $\mP$:
  \begin{equation*}
    \begin{split}
    &\frac{\partial L}{\partial \mP_{B_r,B_r}} \in R^{|B_r| \times |B_r|} \text{ and } \\
    &\mV_{B_r,:}, \frac{\partial L}{\partial \mO_{B_r,:}} \in R^{|B_r| \times d}
    \end{split}
  \end{equation*}

\item Gradient w.r.t. $\mS$:
  \begin{equation*}
    \begin{split}
    &\frac{\partial L}{\partial \mS_{B_r,B_r}}, \frac{\partial L}{\partial \mP_{B_r,B_r}} \text{ and } \mP_{B_r,B_r} \in R^{|B_r| \times |B_r|}
    \end{split}
  \end{equation*}

\item Gradient w.r.t. $\mQ$:
  \begin{equation*}
    \begin{split}
    &\frac{\partial L}{\partial \mQ_{B_r,:}}, \mK_{B_r,:} \in R^{|B_r| \times d}, \text{ and }\\
    &\frac{\partial L}{\partial \mS_{B_r,B_r}} \in R^{|B_r| \times |B_r|}
    \end{split}
  \end{equation*}

\item Gradient w.r.t. $\mK$:
  \begin{equation*}
    \begin{split}
    &\frac{\partial L}{\partial \mK_{B_r,:}}, \mQ_{B_r,:} \in R^{|B_r| \times d} \text{ and }\\
    &\frac{\partial L}{\partial \mS_{B_r,B_r}} \in R^{|B_r| \times |B_r|}
    \end{split}
  \end{equation*}
\end{enumerate}

\item The worst-case memory footprint requires space for three blocks. For a simplified analysis, we assume each takes up to $M/3$ of the fast memory. This gives us the constraints:
  \begin{equation*}
    |B_r|d \leq \frac{M}{3} \text{ and } |B_r|^2 \leq \frac{M}{3}
  \end{equation*}

\item Solving these constraints yields the block size:
  \begin{equation*}
    |B_r| = \min\left(\lfloor \frac{M}{3d} \rfloor, \lfloor \sqrt{\frac{M}{3}} \rfloor\right)
  \end{equation*}

\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Standard Attention Backward Algorithm}
\begin{itemize}
\item Let $N_b = \lceil T / |B_r| \rceil$ be the number of blocks.
\item We present the block-wise algorithm for computing attention gradients while respecting memory constraints.
\end{itemize}
\framebreak
\begin{itemize}
\item Step 1: We compute the gradient with respect to V as follows.
  \begin{itemize}
  \item For $j = 1, \ldots, N_b$ (column blocks of V, where $J_j$ denotes the $j$-th block)
    \begin{itemize}
    \item Initialize $\frac{\partial L}{\partial \mV_{J_j,:}} = 0$
    \item For $i = 1, \ldots, N_b$ (row blocks of P, where $I_i$ denotes the $i$-th block)
      \begin{equation*}
        \frac{\partial L}{\partial \mV_{J_j,:}} \mathrel{+}= \mP_{I_i,J_j}^\top \frac{\partial L}{\partial \mO_{I_i,:}}
      \end{equation*}
    \item Save $\frac{\partial L}{\partial \mV_{J_j,:}}$ to HBM
    \end{itemize}
  \end{itemize}

\framebreak
\item Step 2: We compute the gradient with respect to P as follows.
  \begin{itemize}
  \item For $i = 1, \ldots, N_b$ (row blocks of P)
    \begin{itemize}
    \item Initialize $\frac{\partial L}{\partial \mP_{I_i,:}} = 0$
    \item For $j = 1, \ldots, N_b$ (column blocks of V)
      \begin{equation*}
        \frac{\partial L}{\partial \mP_{I_i,J_j}} = \frac{\partial L}{\partial \mO_{I_i,:}} \mV_{J_j,:}^\top
      \end{equation*}
    \item Save $\frac{\partial L}{\partial \mP_{I_i,:}}$ to HBM
    \end{itemize}
  \end{itemize}

\framebreak
\item Step 3: We compute the gradient with respect to S as follows.
  \begin{itemize}
  \item For $i = 1, \ldots, N_b$
    \begin{itemize}
    \item Compute row-wise sum:
    \begin{equation*}
      \text{sum}_{I_i} = \sum_{j=1}^T \frac{\partial L}{\partial P_{ij}} P_{ij}
    \end{equation*}
    where the operation involves element-wise product and summation.
    \item For $j = 1, \ldots, N_b$
      \begin{equation*}
        \frac{\partial L}{\partial \mS_{I_i,J_j}} = \mP_{I_i,J_j} \odot \left( \frac{\partial L}{\partial \mP_{I_i,J_j}} - \text{sum}_{I_i} \right)
      \end{equation*}
    \item Save $\frac{\partial L}{\partial \mS_{I_i,J_j}}$ to HBM
    \end{itemize}
  \end{itemize}

\framebreak
\item Step 4: We compute the gradient with respect to K as follows.
  \begin{itemize}
  \item For $j = 1, \ldots, N_b$ (column blocks of K)
    \begin{itemize}
    \item Initialize $\frac{\partial L}{\partial \mK_{J_j,:}} = 0$
    \item For $i = 1, \ldots, N_b$ (row blocks of Q)
      \begin{equation*}
        \frac{\partial L}{\partial \mK_{J_j,:}} \mathrel{+}= \mQ_{I_i,:}^\top \frac{\partial L}{\partial \mS_{I_i,J_j}}
      \end{equation*}
    \item Save $\frac{\partial L}{\partial \mK_{J_j,:}}$ to HBM
    \end{itemize}
  \end{itemize}

\framebreak
\item Step 5: We compute the gradient with respect to Q as follows.
  \begin{itemize}
  \item For $i = 1, \ldots, N_b$ (row blocks of Q)
    \begin{itemize}
    \item Initialize $\frac{\partial L}{\partial \mQ_{I_i,:}} = 0$
    \item For $j = 1, \ldots, N_b$ (column blocks of K)
      \begin{equation*}
        \frac{\partial L}{\partial \mQ_{I_i,:}} \mathrel{+}= \frac{\partial L}{\partial \mS_{I_i,J_j}} \mK_{J_j,:}
      \end{equation*}
    \item Save $\frac{\partial L}{\partial \mQ_{I_i,:}}$ to HBM
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Computational Complexity Analysis}
\begin{itemize}
\item The computational complexity for Steps 1, 2, 4, and 5 is:
  \begin{equation*}
    O(N_b) \times O(N_b) \times O(|B_r|^2 \times d) = O(T^2 \times d)
  \end{equation*}
\item The computational complexity for Step 3 is:
  \begin{equation*}
    O(N_b) \times O(N_b) \times O(|B_r|^2) = O(T^2)
  \end{equation*}
  since $N_b \cdot |B_r| = T$.

\item The total computational complexity is dominated by the matrix operations in Steps 1, 2, 4, and 5:
  \begin{equation*}
    O(T^2 \times d)
  \end{equation*}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{I/O Analysis}
\begin{itemize}
\item From the algorithm analysis, the I/O bottleneck arises from reading $\mP$ and writing 
\begin{equation*}
\frac{\partial L}{\partial \mP} \text{ and } \frac{\partial L}{\partial \mS}
\end{equation*}

  \item The total I/O complexity is:
  \begin{equation*}
    O(T^2)
  \end{equation*}
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]
	\frametitle{References}
	\begin{scriptsize}
		\def\newblock{}
		\bibliographystyle{abbrvnat}
		\bibliography{references}
	\end{scriptsize}
\end{frame}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
